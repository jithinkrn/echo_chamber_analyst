"""
LangGraph Node Implementations

This module contains the individual node implementations that replace
the custom agents, providing tools integration, LLM capabilities,
and sophisticated processing logic.
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import json
import re

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.tools import Tool
from langchain_community.tools import DuckDuckGoSearchRun

from .state import (
    EchoChamberAnalystState, ContentItem, Insight, InfluencerProfile,
    ContentType, InsightType, TaskStatus, ProcessingMetrics
)
from .tools import get_tools_for_node, LANGGRAPH_TOOLS
from .monitoring import (
    monitor_node_execution, global_monitor,
    trace_insight_generation, trace_content_filtering
)

# Import real data collection functions
from .scout_data_collection import collect_real_brand_data

# Import Django models for dashboard data
from common.models import Campaign, Community, PainPoint, Influencer, Thread, DashboardMetrics

logger = logging.getLogger(__name__)

# Initialize OpenAI LLM
llm = ChatOpenAI(
    model="gpt-4",
    temperature=0.1,
    max_tokens=2000
)


@monitor_node_execution(global_monitor)
async def scout_node(state: EchoChamberAnalystState) -> EchoChamberAnalystState:
    """
    Real-Time Scout Node - Content Discovery and Dashboard Data Collection

    Uses real web scraping and forum data collection to gather actual brand data
    from Reddit, forums, and review sites. Integrates with SearchUtils for
    comprehensive multi-platform data collection.
    """
    state.current_node = "scout_content"

    try:
        # Extract brand information from campaign
        brand_name = state.campaign.name if hasattr(state.campaign, 'name') else "Unknown Brand"
        brand_keywords = state.campaign.keywords if hasattr(state.campaign, 'keywords') else []
        
        logger.info(f"ðŸ” REAL Scout node processing brand: {brand_name}")
        logger.info(f"ðŸ“‹ Keywords: {', '.join(brand_keywords)}")

        # Get tools for scout operations
        tools = get_tools_for_node("scout")

        # Create enhanced scout prompt for real data collection
        scout_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a real-time content scout agent responsible for comprehensive brand data collection.

            Campaign Context:
            - Brand: {brand_name}
            - Keywords: {keywords}
            - Sources: Reddit, Forums, Review Sites
            - Collection Method: Real-time web scraping and API integration

            Real-Time Capabilities:
            1. Live Reddit subreddit scanning
            2. Forum thread collection and analysis
            3. Review site data extraction
            4. Brand mention detection and tracking
            5. Real pain point extraction from user content
            6. Authentic echo score calculation
            7. Live sentiment analysis
            8. Community health monitoring

            Your tasks:
            1. Collect real brand discussions from multiple platforms
            2. Extract genuine user pain points and feedback
            3. Calculate authentic echo scores from actual content
            4. Identify real community trends and patterns
            5. Monitor brand sentiment across platforms
            6. Store data for live dashboard updates
            7. Track brand mentions and context
            8. Analyze discussion types and engagement

            This is REAL data collection - no simulation or mock data.
            """),
            ("human", "Collect real-time brand data for comprehensive analysis. Brand: {brand_name}")
        ])

        # Format the prompt
        formatted_prompt = scout_prompt.format_messages(
            brand_name=brand_name,
            keywords=", ".join(brand_keywords)
        )

        # **STEP 1: IDENTIFY TOP 4 COMMUNITIES** - Token-efficient community selection
        from agents.scout_data_collection import identify_top_echo_chambers

        logger.info(f"ðŸ” Identifying top 4 echo chambers for: {brand_name}")

        top_communities = await identify_top_echo_chambers(
            brand_name=brand_name,
            keywords=brand_keywords,
            max_communities=4,
            use_cache=True  # Use cached LLM suggestions to save tokens
        )

        # Store selected communities in campaign for tracking
        state.campaign.monitored_communities = top_communities
        state.campaign.collection_weeks = 4
        state.campaign.save()

        logger.info(f"âœ… Selected communities: {[c['name'] for c in top_communities]}")

        # **STEP 2: COLLECT DATA FROM TOP 4 COMMUNITIES ONLY** - 4-week window
        logger.info(f"ðŸš€ Starting REAL brand data collection for: {brand_name}")

        collected_data = await collect_real_brand_data(
            brand_name,
            brand_keywords,
            config={
                'target_communities': [c['name'] for c in top_communities],
                'collection_weeks': 4,
                'use_llm_discovery': False  # Skip LLM discovery, use selected communities
            }
        )

        logger.info(f"ðŸ“Š Real data collected:")
        logger.info(f"  - Communities: {len(collected_data.get('communities', []))}")
        logger.info(f"  - Threads: {len(collected_data.get('threads', []))}")
        logger.info(f"  - Pain Points: {len(collected_data.get('pain_points', []))}")
        logger.info(f"  - Brand Mentions: {len(collected_data.get('brand_mentions', []))}")

        # Store real data in Django models for dashboard
        await _store_real_dashboard_data(collected_data, state.campaign, brand_name)

        # Convert real data to ContentItem format for state management
        discovered_content = []
        for thread in collected_data.get("threads", []):
            discovered_content.append({
                "id": thread["thread_id"],
                "content": thread["content"],
                "source_url": thread.get("url", ""),
                "content_type": "forum_post",
                "author": thread.get("author", "unknown"),
                "title": thread.get("title", "Untitled"),
                "published_at": thread.get("created_at"),
                "echo_score": thread.get("echo_score", 0.5),
                "sentiment_score": thread.get("sentiment_score", 0.0),
                "platform": thread.get("platform", "unknown"),
                "is_real_data": thread.get("is_real_data", True),
                "brand_mentioned": thread.get("brand_mentioned", False)
            })

        # Add discovered real content to state
        for content_data in discovered_content:
            content_item = ContentItem(
                id=content_data["id"],
                content=content_data["content"],
                source_url=content_data["source_url"],
                content_type=ContentType(content_data["content_type"]),
                author=content_data.get("author"),
                title=content_data.get("title"),
                published_at=content_data.get("published_at"),
                echo_score=content_data.get("echo_score", 0.5)
            )
            state.add_content(content_item)

        # Update metrics with real data collection costs
        state.update_metrics(
            tokens=600,  # Higher token usage for real analysis
            cost=0.025,  # Higher cost for real web scraping and LLM analysis
            api_calls=len(collected_data.get("data_sources", []))
        )

        # Create comprehensive audit log for real data collection
        audit_tool = LANGGRAPH_TOOLS["create_audit_log"]
        await audit_tool._arun(
            action_type="real_brand_data_collection",
            action_description=f"REAL Scout collected data for brand '{brand_name}' from {len(collected_data.get('data_sources', []))} real sources",
            agent_name="real_scout_node",
            metadata={
                "brand_name": brand_name,
                "keywords": brand_keywords,
                "data_sources": collected_data.get("data_sources", []),
                "communities_found": len(collected_data.get("communities", [])),
                "threads_collected": len(collected_data.get("threads", [])),
                "pain_points_identified": len(collected_data.get("pain_points", [])),
                "brand_mentions": len(collected_data.get("brand_mentions", [])),
                "collection_method": "real_web_scraping",
                "platforms_searched": ["reddit", "forums", "review_sites"],
                "is_real_data": True,
                "collection_timestamp": collected_data.get("collection_timestamp"),
                "capabilities": [
                    "real_time_scraping",
                    "forum_data_collection", 
                    "reddit_api_integration",
                    "brand_mention_tracking",
                    "authentic_pain_point_extraction",
                    "real_echo_score_calculation",
                    "live_sentiment_analysis",
                    "community_health_monitoring"
                ]
            }
        )

        logger.info(f"âœ… REAL Scout completed - collected {len(discovered_content)} real items for brand '{brand_name}'")
        logger.info(f"ðŸŽ¯ Real capabilities: 8 live data collection capabilities active")

    except Exception as e:
        logger.error(f"âŒ REAL Scout node error: {e}")
        state.add_error(f"REAL Scout node failed: {e}")

    return state


async def _generate_and_store_campaign_insights(collected_data: Dict[str, Any], campaign, brand_name: str) -> None:
    """
    Generate campaign-specific AI insights using OPTIMIZED Analytics Agent.

    OPTIMIZATION: Pre-aggregates data into compact summary BEFORE calling Analytics Agent.
    This reduces token usage by ~90% (from ~10,000 to ~1,000 tokens per insight generation).

    This function delegates to the Analytics Agent (analyst.py) for all insight generation,
    maintaining the agentic architecture where the Analytics Agent owns all insight logic.
    """
    try:
        from django.utils import timezone
        from common.models import Campaign as CampaignModel, Brand
        from asgiref.sync import sync_to_async
        from agents.analyst import generate_campaign_ai_insights

        # Get the Campaign and Brand objects
        campaign_obj = await sync_to_async(CampaignModel.objects.get)(id=campaign.id)
        brand_obj = await sync_to_async(Brand.objects.get)(name=brand_name)

        logger.info(f"ðŸ’¡ Generating OPTIMIZED Campaign AI Insights: '{campaign_obj.name}'...")

        # OPTIMIZATION: Pre-aggregate data (NO LLM, cheap)
        threads = collected_data.get("threads", [])
        pain_points = collected_data.get("pain_points", [])
        communities = collected_data.get("communities", [])

        # Create compact data summary
        data_summary = {
            'total_threads': len(threads),
            'total_communities': len(communities),
            'total_pain_points': len(pain_points),

            # Top pain points (already extracted, no LLM needed)
            'top_pain_points': [
                {
                    'keyword': pp.get('keyword'),
                    'mentions': pp.get('mention_count'),
                    'sentiment': pp.get('sentiment_score'),
                    'week': pp.get('month_year')
                }
                for pp in sorted(pain_points, key=lambda x: x.get('mention_count', 0), reverse=True)[:5]
            ],

            # Sentiment distribution (already calculated)
            'sentiment_breakdown': {
                'positive': len([t for t in threads if t.get('sentiment_score', 0) > 0.2]),
                'negative': len([t for t in threads if t.get('sentiment_score', 0) < -0.2]),
                'neutral': len([t for t in threads if abs(t.get('sentiment_score', 0)) <= 0.2])
            },
            'avg_sentiment': sum(t.get('sentiment_score', 0.0) for t in threads) / len(threads) if threads else 0.0,

            # Top communities (by thread count)
            'top_communities': [
                {
                    'name': c.get('name'),
                    'threads': len([t for t in threads if t.get('community') == c.get('name')]),
                    'echo_score': c.get('echo_score', 0)
                }
                for c in sorted(communities, key=lambda x: x.get('echo_score', 0), reverse=True)[:3]
            ],

            # Sample high-value threads (top 3 by relevance_score)
            'sample_threads': [
                {
                    'title': t.get('title'),
                    'snippet': t.get('content', '')[:150],  # First 150 chars
                    'sentiment': t.get('sentiment_score'),
                    'relevance': t.get('relevance_score', 0),
                    'platform': t.get('platform')
                }
                for t in sorted(threads, key=lambda x: x.get('relevance_score', 0), reverse=True)[:3]
            ]
        }

        # SINGLE OPTIMIZED LLM CALL with compact summary (90% token reduction)
        campaign_insights = await sync_to_async(generate_campaign_ai_insights)(
            campaign=campaign_obj,
            brand=brand_obj,
            collected_data={'data_summary': data_summary}  # Pass summary instead of raw data
        )

        # Calculate overall sentiment label
        avg_sentiment = data_summary['avg_sentiment']
        sentiment_label = "positive" if avg_sentiment > 0.2 else "negative" if avg_sentiment < -0.2 else "neutral"

        # Store insights in campaign metadata
        if not campaign_obj.metadata:
            campaign_obj.metadata = {}

        campaign_obj.metadata['insights'] = campaign_insights
        campaign_obj.metadata['insights_generated_at'] = timezone.now().isoformat()
        campaign_obj.metadata['data_summary'] = {
            'communities': len(communities),
            'threads': len(threads),
            'pain_points': len(pain_points),
            'avg_sentiment': round(avg_sentiment, 2),
            'sentiment_label': sentiment_label,
            'threads_analyzed': len([t for t in threads if t.get('relevance_score', 0) >= 3])  # High-value threads
        }

        # Save using sync_to_async
        await sync_to_async(campaign_obj.save)()

        logger.info(f"âœ… Generated {len(campaign_insights)} Campaign AI Insights (OPTIMIZED)")
        logger.info(f"ðŸ’° Token savings: ~90% vs full thread analysis")

    except Exception as e:
        logger.error(f"âŒ Error generating optimized campaign insights: {e}")
        # Don't raise exception to avoid breaking the workflow


async def _generate_and_store_brand_analytics_insights(collected_data: Dict[str, Any], campaign, brand_name: str) -> None:
    """
    Generate AI-Powered Key Insights for Brand Analytics (automatic campaign).

    This function delegates to the Analytics Agent for generating Brand Analytics insights,
    which are displayed at the top of the dashboard.
    """
    try:
        from django.utils import timezone
        from common.models import Campaign as CampaignModel, Brand
        from asgiref.sync import sync_to_async
        from agents.analyst import generate_ai_powered_insights_from_brand_analytics

        # Get the Campaign and Brand objects
        campaign_obj = await sync_to_async(CampaignModel.objects.get)(id=campaign.id)
        brand_obj = await sync_to_async(Brand.objects.get)(name=brand_name)

        logger.info(f"ðŸ’¡ Delegating to Analytics Agent for Brand Analytics AI-Powered Insights: '{brand_obj.name}'...")

        # Prepare Brand Analytics data for insight generation
        kpis = {
            'total_threads': len(collected_data.get('threads', [])),
            'total_communities': len(collected_data.get('communities', [])),
            'total_pain_points': len(collected_data.get('pain_points', [])),
            'avg_sentiment': sum(t.get('sentiment_score', 0.0) for t in collected_data.get('threads', [])) / max(len(collected_data.get('threads', [])), 1)
        }

        communities = collected_data.get('communities', [])
        pain_points = collected_data.get('pain_points', [])
        influencers = collected_data.get('influencers', [])

        # Call Analytics Agent to generate AI-Powered Key Insights
        ai_insights = await sync_to_async(generate_ai_powered_insights_from_brand_analytics)(
            brand=brand_obj,
            kpis=kpis,
            communities=communities,
            pain_points=pain_points,
            influencers=influencers
        )

        # Store insights in campaign metadata
        if not campaign_obj.metadata:
            campaign_obj.metadata = {}

        campaign_obj.metadata['ai_insights'] = ai_insights  # 6 simple strings
        campaign_obj.metadata['ai_insights_generated_at'] = timezone.now().isoformat()

        await sync_to_async(campaign_obj.save)()

        logger.info(f"âœ… Analytics Agent generated {len(ai_insights)} AI-Powered Key Insights for Brand Analytics")

    except Exception as e:
        logger.error(f"âŒ Error generating Brand Analytics insights: {e}")
        # Don't raise exception to avoid breaking the workflow


async def _generate_and_store_custom_campaign_insights(collected_data: Dict[str, Any], campaign, brand_name: str) -> None:
    """
    Generate Campaign AI Insights for Custom Campaigns using OPTIMIZED approach.
    Takes campaign objectives (from description) into account.

    OPTIMIZATION: Pre-aggregates data into compact summary BEFORE calling Analytics Agent.
    This reduces token usage by ~90% while maintaining insight quality.

    This function delegates to the Analytics Agent for generating Custom Campaign insights,
    which are displayed at the bottom of the dashboard in Campaign Analytics section.
    """
    try:
        from django.utils import timezone
        from common.models import Campaign as CampaignModel, Brand
        from asgiref.sync import sync_to_async
        from agents.analyst import generate_campaign_ai_insights

        # Get the Campaign and Brand objects
        campaign_obj = await sync_to_async(CampaignModel.objects.get)(id=campaign.id)
        brand_obj = await sync_to_async(Brand.objects.get)(name=brand_name)

        logger.info(f"ðŸ’¡ Generating OPTIMIZED Custom Campaign AI Insights: '{campaign_obj.name}'...")
        if campaign_obj.description:
            logger.info(f"ðŸ“‹ Using Campaign Objectives: {campaign_obj.description[:200]}")

        # OPTIMIZATION: Pre-aggregate data (NO LLM, cheap)
        threads = collected_data.get("threads", [])
        pain_points = collected_data.get("pain_points", [])
        communities = collected_data.get("communities", [])

        # Create compact data summary with campaign objectives context
        data_summary = {
            'total_threads': len(threads),
            'total_communities': len(communities),
            'total_pain_points': len(pain_points),
            'campaign_objectives': campaign_obj.description if campaign_obj.description else None,

            # Top pain points (already extracted, no LLM needed)
            'top_pain_points': [
                {
                    'keyword': pp.get('keyword'),
                    'mentions': pp.get('mention_count'),
                    'sentiment': pp.get('sentiment_score'),
                    'week': pp.get('month_year'),
                    'severity': pp.get('severity')
                }
                for pp in sorted(pain_points, key=lambda x: x.get('priority_score', 0), reverse=True)[:5]
            ],

            # Sentiment distribution (already calculated)
            'sentiment_breakdown': {
                'positive': len([t for t in threads if t.get('sentiment_score', 0) > 0.2]),
                'negative': len([t for t in threads if t.get('sentiment_score', 0) < -0.2]),
                'neutral': len([t for t in threads if abs(t.get('sentiment_score', 0)) <= 0.2])
            },
            'avg_sentiment': sum(t.get('sentiment_score', 0.0) for t in threads) / len(threads) if threads else 0.0,

            # Top communities (by echo score)
            'top_communities': [
                {
                    'name': c.get('name'),
                    'threads': len([t for t in threads if t.get('community') == c.get('name')]),
                    'echo_score': c.get('echo_score', 0),
                    'platform': c.get('platform')
                }
                for c in sorted(communities, key=lambda x: x.get('echo_score', 0), reverse=True)[:3]
            ],

            # Sample high-value threads (top 3 by relevance_score)
            'sample_threads': [
                {
                    'title': t.get('title'),
                    'snippet': t.get('content', '')[:150],  # First 150 chars
                    'sentiment': t.get('sentiment_score'),
                    'relevance': t.get('relevance_score', 0),
                    'platform': t.get('platform'),
                    'community': t.get('community')
                }
                for t in sorted(threads, key=lambda x: x.get('relevance_score', 0), reverse=True)[:3]
            ]
        }

        # SINGLE OPTIMIZED LLM CALL with compact summary and objectives (90% token reduction)
        campaign_insights = await sync_to_async(generate_campaign_ai_insights)(
            campaign=campaign_obj,
            brand=brand_obj,
            collected_data={'data_summary': data_summary}  # Pass summary instead of raw data
        )

        # Calculate overall metrics
        avg_sentiment = data_summary['avg_sentiment']
        sentiment_label = "positive" if avg_sentiment > 0.2 else "negative" if avg_sentiment < -0.2 else "neutral"

        # Store insights in campaign metadata
        if not campaign_obj.metadata:
            campaign_obj.metadata = {}

        campaign_obj.metadata['insights'] = campaign_insights  # Structured objects
        campaign_obj.metadata['insights_generated_at'] = timezone.now().isoformat()
        campaign_obj.metadata['data_summary'] = {
            'communities': len(communities),
            'threads': len(threads),
            'pain_points': len(pain_points),
            'avg_sentiment': round(avg_sentiment, 2),
            'sentiment_label': sentiment_label,
            'threads_analyzed': len([t for t in threads if t.get('relevance_score', 0) >= 3])
        }

        # Keep objectives in metadata
        if campaign_obj.description:
            campaign_obj.metadata['objectives'] = campaign_obj.description

        await sync_to_async(campaign_obj.save)()

        logger.info(f"âœ… Generated {len(campaign_insights)} Custom Campaign AI Insights (OPTIMIZED)")
        logger.info(f"ðŸ’° Token savings: ~90% vs full thread analysis")

    except Exception as e:
        logger.error(f"âŒ Error generating Custom Campaign insights: {e}")
        # Don't raise exception to avoid breaking the workflow


def _extract_and_store_influencers(collected_data: Dict[str, Any], campaign, brand_name: str) -> int:
    """
    Extract influencers from thread data and calculate their metrics.

    Uses a 4-component scoring model:
    - Reach Score (0-100): Post volume + engagement + community diversity
    - Authority Score (0-100): Posting consistency + content quality + engagement ratio
    - Advocacy Score (0-100): Brand mention rate + sentiment towards brand
    - Relevance Score (0-100): Brand mention frequency + post volume + community relevance
    """
    try:
        from django.utils import timezone
        from collections import defaultdict

        logger.info(f"ðŸ‘¤ Extracting influencer data from threads...")

        threads = collected_data.get("threads", [])
        if not threads:
            logger.info("No threads to extract influencers from")
            return 0

        # Aggregate data per author
        author_stats = defaultdict(lambda: {
            'username': '',
            'display_name': '',
            'platform': 'reddit',
            'total_posts': 0,
            'total_upvotes': 0,
            'total_comments': 0,
            'communities': set(),
            'brand_mentions': 0,
            'sentiment_scores': [],
            'thread_ids': [],
            'total_echo_score': 0.0
        })

        # Aggregate thread data by author
        for thread in threads:
            author = thread.get('author', 'unknown')
            if author == 'unknown' or author == '[deleted]':
                continue

            stats = author_stats[author]
            stats['username'] = author
            stats['display_name'] = author
            stats['platform'] = thread.get('platform', 'reddit')
            stats['total_posts'] += 1
            stats['total_upvotes'] += thread.get('upvotes', 0)
            stats['total_comments'] += thread.get('reply_count', 0)
            stats['communities'].add(thread.get('community', 'unknown'))
            stats['thread_ids'].append(thread.get('thread_id', ''))
            stats['total_echo_score'] += thread.get('echo_score', 0.0)

            # Check for brand mentions
            content = (thread.get('title', '') + ' ' + thread.get('content', '')).lower()
            if brand_name.lower() in content:
                stats['brand_mentions'] += 1
                stats['sentiment_scores'].append(thread.get('sentiment_score', 0.0))

        # Calculate scores for each influencer
        influencers_created = 0
        for author, stats in author_stats.items():
            if stats['total_posts'] < 1:  # Include all posters (lowered threshold)
                continue

            # Calculate metrics
            avg_post_score = stats['total_upvotes'] / max(stats['total_posts'], 1)
            avg_engagement_rate = stats['total_comments'] / max(stats['total_posts'], 1)
            brand_mention_rate = (stats['brand_mentions'] / max(stats['total_posts'], 1)) * 100
            avg_sentiment = sum(stats['sentiment_scores']) / len(stats['sentiment_scores']) if stats['sentiment_scores'] else 0.0
            avg_echo_score = stats['total_echo_score'] / max(stats['total_posts'], 1)

            # Calculate 4-component scores (0-100 scale)

            # 1. Reach Score: Based on post volume, engagement, and community diversity
            reach_volume = min(stats['total_posts'] / 10 * 50, 50)  # Max 50 points for 10+ posts
            reach_engagement = min(stats['total_comments'] / 20 * 30, 30)  # Max 30 points for 20+ comments
            reach_diversity = min(len(stats['communities']) / 3 * 20, 20)  # Max 20 points for 3+ communities
            reach_score = reach_volume + reach_engagement + reach_diversity

            # 2. Authority Score: Based on consistency, quality, and engagement ratio
            authority_consistency = min(stats['total_posts'] / 5 * 30, 30)  # Max 30 for 5+ posts
            authority_quality = min(avg_post_score / 10 * 40, 40)  # Max 40 for 10+ avg score
            authority_engagement = min(avg_engagement_rate / 5 * 30, 30)  # Max 30 for 5+ avg engagement
            authority_score = authority_consistency + authority_quality + authority_engagement

            # 3. Advocacy Score: Based on brand mention rate and sentiment
            advocacy_mentions = min(brand_mention_rate / 50 * 70, 70)  # Max 70 for 50%+ brand mentions
            advocacy_sentiment = ((avg_sentiment + 1) / 2) * 30  # Convert -1 to 1 range to 0-30 points
            advocacy_score = advocacy_mentions + advocacy_sentiment

            # 4. Relevance Score: Based on brand mention frequency, post volume, and echo score
            relevance_frequency = min(stats['brand_mentions'] / 3 * 40, 40)  # Max 40 for 3+ brand mentions
            relevance_volume = min(stats['total_posts'] / 10 * 30, 30)  # Max 30 for 10+ posts
            relevance_echo = min(avg_echo_score * 10 * 30, 30)  # Max 30 for 3.0+ avg echo
            relevance_score = relevance_frequency + relevance_volume + relevance_echo

            # Overall influence score (weighted combination: 30% reach, 30% authority, 20% advocacy, 20% relevance)
            influence_score = (
                reach_score * 0.30 +
                authority_score * 0.30 +
                advocacy_score * 0.20 +
                relevance_score * 0.20
            )

            # Get brand object and campaign
            from common.models import Brand, Campaign as CampaignModel
            brand = Brand.objects.filter(name=brand_name).first()
            campaign_obj = CampaignModel.objects.get(id=campaign.id)

            # Get primary community for influencer (most posts in)
            # Filter by brand and campaign to ensure correct community
            community = None
            if stats['communities']:
                # Find the community where this influencer is most active
                community = Community.objects.filter(
                    name__in=list(stats['communities']),
                    brand=brand,
                    campaign=campaign_obj
                ).first()

            # If no community found, skip this influencer or use None
            if not community:
                logger.warning(f"No community found for influencer {author}, communities: {list(stats['communities'])}")

            # Store influencer
            influencer, created = Influencer.objects.update_or_create(
                campaign=campaign_obj,
                username=stats['username'],
                platform=stats['platform'],
                defaults={
                    'brand': brand,
                    'community': community,
                    'display_name': stats['display_name'],
                    'profile_url': f"https://reddit.com/user/{stats['username']}" if stats['platform'] == 'reddit' else '',
                    'total_posts': stats['total_posts'],
                    'total_comments': stats['total_comments'],
                    'total_karma': stats['total_upvotes'],
                    'avg_post_score': round(avg_post_score, 2),
                    'avg_engagement_rate': round(avg_engagement_rate, 2),
                    'reach_score': round(reach_score, 2),
                    'authority_score': round(authority_score, 2),
                    'advocacy_score': round(advocacy_score, 2),
                    'relevance_score': round(relevance_score, 2),
                    'influence_score': round(influence_score, 2),
                    'sentiment_towards_brand': round(avg_sentiment, 2),
                    'brand_mention_count': stats['brand_mentions'],
                    'brand_mention_rate': round(brand_mention_rate, 2),
                    'communities': list(stats['communities']),
                    'last_active': timezone.now()
                }
            )

            if created:
                influencers_created += 1
                logger.debug(f"Created influencer: {stats['username']} (influence: {influence_score:.1f})")
            else:
                logger.debug(f"Updated influencer: {stats['username']} (influence: {influence_score:.1f})")

        logger.info(f"âœ… Extracted {influencers_created} new influencers, updated {len(author_stats) - influencers_created}")
        return len(author_stats)

    except Exception as e:
        logger.error(f"âŒ Error extracting influencers: {e}")
        return 0


# ============================================================================
# RESILIENT DATA SAVING - Phase 5
# ============================================================================

from typing import Callable
from dataclasses import dataclass, field

@dataclass
class SaveResult:
    """
    Track results of save operations for monitoring and resilience.

    Provides detailed statistics about what succeeded vs failed during
    bulk save operations.
    """
    total: int = 0
    succeeded: int = 0
    failed: int = 0
    created: int = 0
    updated: int = 0
    errors: List[Dict[str, Any]] = field(default_factory=list)

    def add_success(self, created: bool = False):
        """Record successful save"""
        self.succeeded += 1
        if created:
            self.created += 1
        else:
            self.updated += 1

    def add_failure(self, item_id: str, error: str, item_data: Dict = None):
        """Record failed save"""
        self.failed += 1
        from django.utils import timezone
        self.errors.append({
            'item_id': item_id,
            'error': str(error),
            'item_data': item_data if item_data and len(str(item_data)) < 500 else str(item_data)[:500] if item_data else None,
            'timestamp': timezone.now().isoformat()
        })

    def get_summary(self) -> Dict[str, Any]:
        """Get summary statistics"""
        return {
            'total': self.total,
            'succeeded': self.succeeded,
            'failed': self.failed,
            'created': self.created,
            'updated': self.updated,
            'success_rate': round(self.succeeded / self.total * 100, 1) if self.total > 0 else 0,
            'error_count': len(self.errors)
        }

    def log_summary(self, item_type: str):
        """Log summary of save operation"""
        summary = self.get_summary()
        if self.failed > 0:
            logger.warning(f"âš ï¸ {item_type} save completed with errors:")
        else:
            logger.info(f"âœ… {item_type} save completed successfully:")

        logger.info(f"   Total: {self.total}")
        logger.info(f"   Succeeded: {self.succeeded} ({summary['success_rate']}%)")
        logger.info(f"   Created: {self.created}, Updated: {self.updated}")
        if self.failed > 0:
            logger.info(f"   Failed: {self.failed}")
            # Log first few errors
            for error in self.errors[:3]:
                logger.error(f"      - {error['item_id']}: {error['error']}")
            if len(self.errors) > 3:
                logger.error(f"      ... and {len(self.errors) - 3} more errors")


def resilient_bulk_save(
    items: List[Any],
    save_function: Callable,
    item_type: str,
    get_item_id: Callable[[Any], str] = None
) -> SaveResult:
    """
    Save items with granular error handling - each item saves independently.

    If one item fails, others continue. Returns detailed statistics about
    successes and failures.

    Args:
        items: List of items to save
        save_function: Function that saves a single item, returns (object, created)
        item_type: Type name for logging (e.g., "Community", "Thread")
        get_item_id: Optional function to extract item ID for error reporting

    Returns:
        SaveResult with detailed statistics

    Example:
        def save_community(data):
            return Community.objects.get_or_create(name=data['name'], defaults={...})

        result = resilient_bulk_save(
            items=community_data_list,
            save_function=save_community,
            item_type="Community",
            get_item_id=lambda x: x.get('name', 'unknown')
        )
    """
    result = SaveResult(total=len(items))

    logger.info(f"ðŸ’¾ Starting resilient save for {len(items)} {item_type} items...")

    for idx, item in enumerate(items):
        try:
            # Get item ID for error reporting
            if get_item_id:
                item_id = get_item_id(item)
            else:
                item_id = f"{item_type}_{idx}"

            # Attempt save
            obj, created = save_function(item)
            result.add_success(created=created)

            logger.debug(f"   âœ“ {item_type} {item_id}: {'created' if created else 'updated'}")

        except Exception as e:
            # Record failure but continue
            item_id = get_item_id(item) if get_item_id else f"{item_type}_{idx}"
            result.add_failure(
                item_id=item_id,
                error=str(e),
                item_data=item
            )
            logger.error(f"   âœ— {item_type} {item_id} failed: {e}")

    # Log summary
    result.log_summary(item_type)

    return result


def _store_real_dashboard_data(collected_data: Dict[str, Any], campaign, brand_name: str) -> Dict[str, SaveResult]:
    """
    Store real collected data in Django models with RESILIENT error handling.

    Each item (community, thread, pain point) is saved independently.
    If one fails, others continue. Returns detailed save statistics.
    """
    try:
        import gc
        from django.utils import timezone

        logger.info(f"ðŸ’¾ Starting RESILIENT data storage for brand: {brand_name}")

        # Track results for all save operations
        save_results = {
            'communities': None,
            'pain_points': None,
            'threads': None,
            'influencers': None
        }

        # ==================== SAVE COMMUNITIES (RESILIENT) ====================

        def save_community(community_data):
            """Save single community with error handling"""
            community, created = Community.objects.get_or_create(
                name=community_data["name"],
                platform=community_data["platform"],
                defaults={
                    "url": community_data.get("url", f"https://reddit.com/{community_data["name"]}"),
                    "member_count": community_data["member_count"],
                    "echo_score": community_data["echo_score"],
                    "echo_score_change": community_data.get("echo_score_change", 0.0),
                    "description": f"Real community data for {brand_name}",
                    "is_active": True,
                    "last_analyzed": timezone.now(),
                    "category": community_data.get("category", "general"),
                    "language": community_data.get("language", "en")
                }
            )

            if not created:
                # Update existing community with real data
                old_echo_score = community.echo_score or 0.0
                new_echo_score = community_data["echo_score"]

                community.echo_score = new_echo_score
                community.echo_score_change = round(
                    ((new_echo_score - old_echo_score) / max(old_echo_score, 0.1)) * 100, 1
                )
                community.member_count = community_data["member_count"]
                community.last_analyzed = timezone.now()
                community.save()

            return (community, created)

        communities_data = collected_data.get("communities", [])
        if communities_data:
            save_results['communities'] = resilient_bulk_save(
                items=communities_data,
                save_function=save_community,
                item_type="Community",
                get_item_id=lambda x: x.get('name', 'unknown')
            )

        # ==================== SAVE PAIN POINTS (RESILIENT) ====================

        def save_pain_point(pain_point_data):
            """Save single pain point with error handling"""
            # Get first available community (for pain points without specific community)
            community = Community.objects.filter(
                platform__in=["reddit", "forum", "tech_forums", "review_sites"]
            ).first()

            if not community:
                raise ValueError("No community found for pain point storage")

            # Get week number from pain point data
            week_num = pain_point_data.get("month_year", 4)

            return PainPoint.objects.update_or_create(
                keyword=pain_point_data["keyword"],
                campaign_id=campaign.id,
                community=community,
                month_year=week_num,  # Include in lookup for proper weekly separation
                defaults={
                    "mention_count": pain_point_data["mention_count"],
                    "growth_percentage": pain_point_data["growth_percentage"],
                    "sentiment_score": pain_point_data["sentiment_score"],
                    "heat_level": pain_point_data["heat_level"],
                    "example_content": pain_point_data.get("example", "")[:500],
                    "related_keywords": pain_point_data.get("related_keywords", []),
                    "first_seen": timezone.now(),
                    "last_seen": timezone.now()
                }
            )

        pain_points_data = collected_data.get("pain_points", [])
        if pain_points_data:
            save_results['pain_points'] = resilient_bulk_save(
                items=pain_points_data,
                save_function=save_pain_point,
                item_type="PainPoint",
                get_item_id=lambda x: f"{x.get('keyword', 'unknown')}_week{x.get('month_year', 0)}"
            )

        # ==================== SAVE THREADS (RESILIENT) ====================

        from datetime import timedelta
        from agents.scout_data_collection import calculate_month_year


        def save_thread(thread_data):
            """Save single thread with error handling"""
            # Find or create community
            community = Community.objects.filter(
                name=thread_data.get("community")
            ).first()

            if not community:
                # Auto-create missing community
                logger.warning(f"Community '{thread_data.get('community')}' not found, attempting to create...")
                community, _ = Community.objects.get_or_create(
                    name=thread_data.get("community", "Unknown"),
                    platform=thread_data.get("platform", "unknown"),
                    defaults={
                        "url": f"https://reddit.com/r/{thread_data.get('community', 'unknown')}",
                        "member_count": 0,
                        "echo_score": 0.0,
                        "description": "Auto-created community",
                        "is_active": True
                    }
                )

            # Calculate tokens
            thread_tokens = len(thread_data.get("content", "")) // 4

            # Parse published_at
            published_at_raw = thread_data.get("created_at", timezone.now())
            if isinstance(published_at_raw, str):
                from dateutil import parser as date_parser
                try:
                    published_at = date_parser.parse(published_at_raw)
                    if published_at.tzinfo is None:
                        published_at = timezone.make_aware(published_at)
                except Exception:
                    published_at = timezone.now()
            else:
                published_at = published_at_raw

            week_num = calculate_month_year(published_at)

            return Thread.objects.update_or_create(
                thread_id=thread_data["thread_id"],
                defaults={
                    "title": thread_data["title"],
                    "content": thread_data["content"][:2000],
                    "community": community,
                    "campaign_id": campaign.id,
                    "author": thread_data.get("author", "unknown"),
                    "comment_count": thread_data.get("reply_count", 0),
                    "upvotes": thread_data.get("upvotes", 0),
                    "echo_score": thread_data.get("echo_score", 0.0),
                    "sentiment_score": thread_data.get("sentiment_score", 0.0),
                    "published_at": published_at,
                    "analyzed_at": timezone.now(),
                    "month_year": week_num,  # CRITICAL: Tag with week 1-4
                    "token_count": thread_tokens,
                    "processing_cost": thread_tokens * 0.00001
                }
            )

        threads_data = collected_data.get("threads", [])
        if threads_data:
            save_results['threads'] = resilient_bulk_save(
                items=threads_data,
                save_function=save_thread,
                item_type="Thread",
                get_item_id=lambda x: x.get('thread_id', 'unknown')
            )

        # ==================== SAVE INFLUENCERS (RESILIENT) ====================

        try:
            influencer_count = _extract_and_store_influencers(collected_data, campaign, brand_name)
            save_results['influencers'] = SaveResult(
                total=influencer_count,
                succeeded=influencer_count,
                created=influencer_count
            )
        except Exception as e:
            logger.error(f"âŒ Influencer extraction failed: {e}")
            save_results['influencers'] = SaveResult(total=0, failed=1)
            save_results['influencers'].add_failure('all_influencers', str(e))

        # ==================== GENERATE INSIGHTS (RESILIENT) ====================

        import asyncio
        import concurrent.futures

        try:
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    lambda: asyncio.run(_generate_and_store_campaign_insights(collected_data, campaign, brand_name))
                )
                future.result(timeout=30)  # 30 second timeout
            logger.info("âœ… Campaign insights generated successfully")
        except Exception as e:
            logger.error(f"âŒ Campaign insight generation failed (non-critical): {e}")
            # Don't fail the entire save if insights fail

        # ==================== LOG FINAL SUMMARY ====================

        logger.info(f"")
        logger.info(f"{'='*60}")
        logger.info(f"ðŸ“Š RESILIENT DATA SAVE COMPLETE - Brand: '{brand_name}'")
        logger.info(f"{'='*60}")

        total_succeeded = 0
        total_failed = 0

        for data_type, result in save_results.items():
            if result:
                summary = result.get_summary()
                total_succeeded += result.succeeded
                total_failed += result.failed

                status_icon = "âœ…" if result.failed == 0 else "âš ï¸"
                logger.info(f"{status_icon} {data_type.title()}: {result.succeeded}/{result.total} saved ({summary['success_rate']}%)")

                if result.failed > 0:
                    logger.warning(f"   â””â”€ {result.failed} failures logged for retry")

        logger.info(f"")
        logger.info(f"Overall: {total_succeeded} items saved, {total_failed} failures")

        if total_failed > 0:
            logger.warning(f"âš ï¸ Partial save completed with {total_failed} failures")
            logger.warning(f"   Failed items logged for manual review/retry")
        else:
            logger.info(f"âœ… Perfect save - all items succeeded!")

        logger.info(f"{'='*60}")

        # Store save results in campaign metadata for monitoring
        if hasattr(campaign, 'metadata') and campaign.metadata:
            campaign.metadata['last_save_results'] = {
                data_type: result.get_summary() if result else None
                for data_type, result in save_results.items()
            }
            campaign.metadata['last_save_timestamp'] = timezone.now().isoformat()
            try:
                campaign.save()
            except Exception as e:
                logger.error(f"Failed to save campaign metadata: {e}")

        # Final memory cleanup
        gc.collect()
        logger.info("ðŸ§¹ Memory cleanup complete")

        return save_results

    except Exception as e:
        logger.error(f"âŒ Critical error in data storage: {e}")
        # Even if outer function fails, log what we can
        return {
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


def store_brand_analytics_data(collected_data: Dict[str, Any], brand, automatic_campaign) -> None:
    """
    Store data for Brand Analytics (automatic campaign) ONLY.
    Links all data to brand and automatic campaign.

    Args:
        collected_data: Data collected by scout agent
        brand: Brand object
        automatic_campaign: Automatic campaign object
    """
    try:
        import gc
        from django.utils import timezone

        logger.info(f"ðŸ’¾ Storing Brand Analytics data for brand: {brand.name}")

        # Memory optimization: Cache community lookups to avoid repeated queries
        community_cache = {}

        # Store communities (link to brand + automatic campaign)
        for community_data in collected_data.get("communities", []):
            # Generate URL if not provided
            community_name = community_data["name"]
            platform = community_data["platform"]

            if "url" in community_data:
                community_url = community_data["url"]
            else:
                # Generate URL based on platform and name
                if platform == "reddit" and community_name.startswith("r/"):
                    community_url = f"https://reddit.com/{community_name}"
                elif platform == "reddit":
                    community_url = f"https://reddit.com/r/{community_name}"
                else:
                    community_url = f"https://{community_name}"

            community, created = Community.objects.get_or_create(
                name=community_name,
                platform=platform,
                brand=brand,  # Link to brand
                campaign=automatic_campaign,  # Link to automatic campaign
                defaults={
                    "url": community_url,
                    "member_count": community_data.get("member_count", 0),
                    "echo_score": community_data.get("echo_score", 0.0),
                    "echo_score_change": community_data.get("echo_score_change", 0.0),
                    "description": f"Brand Analytics - Community for {brand.name}",
                    "is_active": True,
                    "last_analyzed": timezone.now(),
                    "category": community_data.get("category", "general"),
                    "language": community_data.get("language", "en"),
                    "activity_score": community_data.get("activity_score", 0.0),
                    "threads_last_4_weeks": community_data.get("threads_last_4_weeks", 0),
                    "avg_engagement_rate": community_data.get("avg_engagement_rate", 0.0),
                    "echo_score_delta": community_data.get("echo_score_delta", 0.0)
                }
            )

            if not created:
                # Update existing community with new data
                old_echo_score = community.echo_score or 0.0
                new_echo_score = community_data.get("echo_score", 0.0)

                community.echo_score = new_echo_score
                community.echo_score_change = round(
                    ((new_echo_score - old_echo_score) / max(old_echo_score, 0.1)) * 100, 1
                )
                community.member_count = community_data.get("member_count", community.member_count)
                community.last_analyzed = timezone.now()
                community.activity_score = community_data.get("activity_score", community.activity_score)
                community.save()

            # Cache community for later lookups (memory optimization)
            community_cache[community_data["name"].lower()] = community

        # Store pain points extracted from threads (link each to correct community)
        # Group threads by community to extract community-specific pain points
        import re
        from collections import defaultdict
        community_threads = defaultdict(list)

        for thread_data in collected_data.get("threads", []):
            community_name = thread_data.get("community")
            if community_name:
                community_threads[community_name].append(thread_data)

        # Extract pain points per community from their threads
        for community_name, threads in community_threads.items():
            # Try exact match first
            community = Community.objects.filter(
                name=community_name,
                brand=brand,
                campaign=automatic_campaign
            ).first()

            # If not found, try case-insensitive match
            if not community:
                community = Community.objects.filter(
                    name__iexact=community_name,
                    brand=brand,
                    campaign=automatic_campaign
                ).first()

            # If still not found, try removing/adding domain suffix
            if not community and '.' in community_name:
                base_name = community_name.split('.')[0]
                community = Community.objects.filter(
                    name__iexact=base_name,
                    brand=brand,
                    campaign=automatic_campaign
                ).first()

            if not community and '.' not in community_name:
                for suffix in ['.com', '.org', '.net']:
                    community = Community.objects.filter(
                        name__iexact=f"{community_name}{suffix}",
                        brand=brand,
                        campaign=automatic_campaign
                    ).first()
                    if community:
                        break

            if not community:
                logger.warning(f"Community '{community_name}' not found for pain point extraction (tried variations)")
                continue

            # Define expanded pain point patterns with flexible natural language matching
            pain_point_patterns = {
                "quality_issues": r'\b(?:quality|poor|cheap|flimsy|break(?:s|ing)?|broken|defective|falls? apart|terrible|awful|bad|worse|worst|decrease|thin|weak)\b',
                "sizing_problems": r'\b(?:siz(?:e|ing)|fit(?:s|ting)?|too (?:small|large|big|tight)|runs? (?:small|big)|doesn\'?t fit|wrong size|narrow|wide)\b',
                "durability": r'\b(?:durability|durables?|wear(?:ing)? out|wears? out|doesn\'?t last|fading|faded|worn|tear(?:ing)?|ripped|falling apart)\b',
                "customer_service": r'\b(?:customer service|support|unhelpful|slow response|rude|no reply|ignored|terrible service|bad service|never respond)\b',
                "shipping": r'\b(?:shipping|delivery|late|delayed|never arrived|lost|damaged|slow shipping|took forever)\b',
                "price_value": r'\b(?:expensive|overpriced|too (?:much|expensive)|costly|not worth|waste of money|ripoff|too pricey|price)\b',
                "comfort": r'\b(?:uncomfortable|uncomfy|hurts?|painful|tight|stiff|irritat(?:ing|ed)|chafing|annoying|bothers?)\b',
                "design_issues": r'\b(?:design|ugly|looks? bad|poorly designed|weird|strange|odd|hideous|unattractive)\b',
                "material_problems": r'\b(?:material|fabric|leather|plastic|rubber|feels? bad|texture|scratchy|itchy|synthetic)\b',
                "performance": r'\b(?:performance|doesn\'?t work|failed|useless|ineffective|disappointing|disappointed|not good|doesn\'?t help)\b'
            }

            # Group threads by week for pain point extraction
            from datetime import datetime, timedelta
            from dateutil import parser as date_parser

            now = timezone.now()
            four_weeks_ago = now - timedelta(weeks=4)

            # Group threads by week (1-4)
            threads_by_week = {1: [], 2: [], 3: [], 4: []}
            for thread in threads:
                thread_date_str = thread.get('created_at')
                if thread_date_str:
                    try:
                        # Parse ISO format datetime string
                        thread_date = date_parser.parse(thread_date_str)

                        # Make timezone-aware if needed
                        if thread_date.tzinfo is None:
                            thread_date = timezone.make_aware(thread_date)

                        # Calculate which week this thread belongs to (1-4)
                        # Week 1 = oldest (3-4 weeks ago), Week 4 = newest (0-1 weeks ago)
                        days_ago = (now - thread_date).days

                        if days_ago < 0:
                            week_num = 4  # Future date, put in current week
                        elif days_ago < 7:
                            week_num = 4  # This week
                        elif days_ago < 14:
                            week_num = 3  # Last week
                        elif days_ago < 21:
                            week_num = 2  # 2 weeks ago
                        else:
                            week_num = 1  # 3+ weeks ago

                        threads_by_week[week_num].append(thread)
                        logger.debug(f"Thread dated {thread_date} ({days_ago} days ago) â†’ Week {week_num}")
                    except Exception as e:
                        logger.warning(f"Failed to parse thread date '{thread_date_str}': {e}, defaulting to week 4")
                        threads_by_week[4].append(thread)
                else:
                    threads_by_week[4].append(thread)

            # Extract pain points for each week separately
            for week_num, week_threads in threads_by_week.items():
                if not week_threads:
                    continue

                # Combine content for this week
                week_content = []
                for thread in week_threads:
                    content = f"{thread.get('title', '')} {thread.get('content', '')}"
                    week_content.append(content)

                combined_content = " ".join(week_content).lower()

                # Extract pain points from this week's content
                for keyword, pattern in pain_point_patterns.items():
                    matches = re.findall(pattern, combined_content, re.IGNORECASE)

                    if matches:
                        mention_count = len(matches)

                        # Calculate metrics
                        growth_percentage = min(mention_count * 10, 100)
                        heat_level = min(mention_count // 2 + 1, 5)
                        sentiment_score = -0.3  # Default negative for pain points

                        # Create separate pain point record for each week
                        PainPoint.objects.update_or_create(
                            keyword=keyword.replace("_", " ").title(),
                            campaign=automatic_campaign,
                            brand=brand,
                            community=community,
                            month_year=week_num,  # Unique per week (now in DB constraint)
                            defaults={
                                "mention_count": mention_count,
                                "growth_percentage": growth_percentage,
                                "sentiment_score": sentiment_score,
                                "heat_level": heat_level,
                                "example_content": matches[0][:500] if matches else "",
                                "related_keywords": matches[:3],
                                "first_seen": timezone.now(),
                                "last_seen": timezone.now()
                            }
                        )

        # Memory cleanup: Delete large objects and force garbage collection
        del community_threads
        gc.collect()
        logger.info("ðŸ§¹ Memory cleanup after pain point extraction")

        # Store threads (link to brand + automatic campaign)
        from datetime import timedelta
        from agents.scout_data_collection import calculate_month_year


        # Helper function to get community from cache or DB (memory optimization)
        def get_community_cached(community_name: str):
            """Get community from cache to avoid repeated DB queries."""
            if not community_name:
                return None

            # Try cache first (case-insensitive)
            cache_key = community_name.lower()
            if cache_key in community_cache:
                return community_cache[cache_key]

            # Try variations in cache
            if '.' in community_name:
                base_name = community_name.split('.')[0].lower()
                if base_name in community_cache:
                    return community_cache[base_name]
            else:
                for suffix in ['.com', '.org', '.net']:
                    variant = f"{community_name}{suffix}".lower()
                    if variant in community_cache:
                        return community_cache[variant]

            # Not in cache, query DB (rare case)
            community = Community.objects.filter(
                name__iexact=community_name,
                brand=brand,
                campaign=automatic_campaign
            ).first()

            if community:
                community_cache[cache_key] = community

            return community

        for thread_data in collected_data.get("threads", []):
            # Use cached community lookup (avoids 4-5 DB queries per thread)
            thread_community_name = thread_data.get("community", "")
            community = get_community_cached(thread_community_name)

            if community:
                thread_tokens = len(thread_data.get("content", "")) // 4
                published_at_raw = thread_data.get("created_at", timezone.now())

                # Parse published_at
                if isinstance(published_at_raw, str):
                    from dateutil import parser as date_parser
                    try:
                        published_at = date_parser.parse(published_at_raw)
                        if published_at.tzinfo is None:
                            published_at = timezone.make_aware(published_at)
                    except Exception:
                        published_at = timezone.now()
                else:
                    published_at = published_at_raw

                week_num = calculate_month_year(published_at)

                # Use unique_together fields (thread_id, community) for lookup
                Thread.objects.update_or_create(
                    thread_id=thread_data["thread_id"],
                    community=community,
                    defaults={
                        "title": thread_data["title"],
                        "content": thread_data["content"][:2000],
                        "campaign": automatic_campaign,
                        "brand": brand,  # Link to brand
                        "author": thread_data.get("author", "unknown"),
                        "comment_count": thread_data.get("reply_count", 0),
                        "upvotes": thread_data.get("upvotes", 0),
                        "echo_score": thread_data.get("echo_score", 0.0),
                        "sentiment_score": thread_data.get("sentiment_score", 0.0),
                        "published_at": published_at,
                        "analyzed_at": timezone.now(),
                        "month_year": week_num,
                        "token_count": thread_tokens,
                        "processing_cost": thread_tokens * 0.00001
                    }
                )

        # Store influencers (link to brand + automatic campaign)
        influencer_count = _extract_and_store_influencers(collected_data, automatic_campaign, brand.name)

        # Generate Brand Analytics insights using LLM
        import asyncio
        import concurrent.futures

        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(
                lambda: asyncio.run(_generate_and_store_brand_analytics_insights(collected_data, automatic_campaign, brand.name))
            )
            future.result()

        # Final memory cleanup
        del community_cache
        gc.collect()
        logger.info("ðŸ§¹ Final memory cleanup complete")

        logger.info(f"âœ… Brand Analytics data stored successfully for brand '{brand.name}'")
        logger.info(f"ðŸ“Š Stored: {len(collected_data.get('communities', []))} communities, "
                   f"{len(collected_data.get('pain_points', []))} pain points, "
                   f"{len(collected_data.get('threads', []))} threads, "
                   f"{influencer_count} influencers")

    except Exception as e:
        logger.error(f"âŒ Error storing Brand Analytics data: {e}", exc_info=True)


def store_custom_campaign_data(collected_data: Dict[str, Any], brand, campaign) -> None:
    """
    Store data for Custom Campaign ONLY.
    Links all data to the specific custom campaign.
    Takes campaign objectives (from description) into account.

    Args:
        collected_data: Data collected by scout agent
        brand: Brand object
        campaign: Custom campaign object
    """
    try:
        from django.utils import timezone

        logger.info(f"ðŸ’¾ Storing Custom Campaign data for campaign: {campaign.name}")
        logger.info(f"ðŸ“‹ Campaign Objectives: {campaign.description[:100] if campaign.description else 'None'}")

        # Store campaign objectives in metadata for reference
        if campaign.description:
            if not campaign.metadata:
                campaign.metadata = {}
            campaign.metadata['objectives'] = campaign.description
            campaign.save()

        # Store communities (link to this campaign)
        for community_data in collected_data.get("communities", []):
            community, created = Community.objects.get_or_create(
                name=community_data["name"],
                platform=community_data["platform"],
                brand=brand,  # Link to brand
                campaign=campaign,  # Link to THIS campaign
                defaults={
                    "url": community_data.get("url", f"https://reddit.com/{community_data["name"]}"),
                    "member_count": community_data["member_count"],
                    "echo_score": community_data["echo_score"],
                    "echo_score_change": community_data.get("echo_score_change", 0.0),
                    "description": f"Custom Campaign: {campaign.name} - {community_data['name']}",
                    "is_active": True,
                    "last_analyzed": timezone.now(),
                    "category": community_data.get("category", "custom_campaign"),
                    "language": community_data.get("language", "en"),
                    "activity_score": community_data.get("activity_score", 0.0),
                    "threads_last_4_weeks": community_data.get("threads_last_4_weeks", 0),
                    "avg_engagement_rate": community_data.get("avg_engagement_rate", 0.0),
                    "echo_score_delta": community_data.get("echo_score_delta", 0.0)
                }
            )

            if not created:
                old_echo_score = community.echo_score or 0.0
                new_echo_score = community_data["echo_score"]

                community.echo_score = new_echo_score
                community.echo_score_change = round(
                    ((new_echo_score - old_echo_score) / max(old_echo_score, 0.1)) * 100, 1
                )
                community.member_count = community_data["member_count"]
                community.last_analyzed = timezone.now()
                community.save()

        # Store pain points (link to this campaign with objectives context)
        for pain_point_data in collected_data.get("pain_points", []):
            community = Community.objects.filter(
                brand=brand,
                campaign=campaign,
                platform__in=["reddit", "forum", "tech_forums", "review_sites"]
            ).first()

            if community:
                week_num = pain_point_data.get("month_year", 4)

                PainPoint.objects.update_or_create(
                    keyword=pain_point_data["keyword"],
                    campaign=campaign,
                    brand=brand,  # Link to brand
                    community=community,
                    defaults={
                        "mention_count": pain_point_data["mention_count"],
                        "growth_percentage": pain_point_data["growth_percentage"],
                        "sentiment_score": pain_point_data["sentiment_score"],
                        "heat_level": pain_point_data["heat_level"],
                        "month_year": week_num,
                        "example_content": pain_point_data.get("example", "")[:500],
                        "related_keywords": pain_point_data.get("related_keywords", []),
                        "first_seen": timezone.now(),
                        "last_seen": timezone.now()
                    }
                )

        # Store threads (link to this campaign)
        from datetime import timedelta
        from agents.scout_data_collection import calculate_month_year


        for thread_data in collected_data.get("threads", []):
            community = Community.objects.filter(
                name=thread_data.get("community"),
                brand=brand,
                campaign=campaign
            ).first()

            if community:
                thread_tokens = len(thread_data.get("content", "")) // 4
                published_at_raw = thread_data.get("created_at", timezone.now())

                # Parse published_at
                if isinstance(published_at_raw, str):
                    from dateutil import parser as date_parser
                    try:
                        published_at = date_parser.parse(published_at_raw)
                        if published_at.tzinfo is None:
                            published_at = timezone.make_aware(published_at)
                    except Exception:
                        published_at = timezone.now()
                else:
                    published_at = published_at_raw

                week_num = calculate_month_year(published_at)

                Thread.objects.update_or_create(
                    thread_id=thread_data["thread_id"],
                    campaign=campaign,
                    brand=brand,  # Link to brand
                    defaults={
                        "title": thread_data["title"],
                        "content": thread_data["content"][:2000],
                        "community": community,
                        "author": thread_data.get("author", "unknown"),
                        "comment_count": thread_data.get("reply_count", 0),
                        "upvotes": thread_data.get("upvotes", 0),
                        "echo_score": thread_data.get("echo_score", 0.0),
                        "sentiment_score": thread_data.get("sentiment_score", 0.0),
                        "published_at": published_at,
                        "analyzed_at": timezone.now(),
                        "month_year": week_num,
                        "token_count": thread_tokens,
                        "processing_cost": thread_tokens * 0.00001
                    }
                )

        # Store influencers (link to this campaign)
        influencer_count = _extract_and_store_influencers(collected_data, campaign, brand.name)

        # Generate Custom Campaign insights using LLM (with campaign objectives)
        import asyncio
        import concurrent.futures

        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(
                lambda: asyncio.run(_generate_and_store_custom_campaign_insights(
                    collected_data, campaign, brand.name
                ))
            )
            future.result()

        logger.info(f"âœ… Custom Campaign data stored successfully for campaign '{campaign.name}'")
        logger.info(f"ðŸ“Š Stored: {len(collected_data.get('communities', []))} communities, "
                   f"{len(collected_data.get('pain_points', []))} pain points, "
                   f"{len(collected_data.get('threads', []))} threads, "
                   f"{influencer_count} influencers")

    except Exception as e:
        logger.error(f"âŒ Error storing Custom Campaign data: {e}", exc_info=True)


# Remove all the old simulated data collection functions that were duplicated
# (collect_reddit_data, collect_discord_data, collect_tiktok_data, etc.)
# Keep only the essential node implementations


@monitor_node_execution(global_monitor)
async def cleaner_node(state: EchoChamberAnalystState) -> EchoChamberAnalystState:
    """
    Enhanced Data Cleaner Node - Advanced Content Cleaning and Dashboard Data Processing

    Replaces the Data Cleaner Agent with PII detection, spam filtering,
    content validation, compliance tracking, and dashboard-specific cleaning.
    """
    state.current_node = "clean_content"

    try:
        logger.info(f"Enhanced Cleaner node processing {len(state.raw_content)} items")

        # Get tools for cleaning operations
        tools = get_tools_for_node("cleaner")

        # Create enhanced cleaner prompt
        cleaner_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an enhanced data cleaning agent responsible for comprehensive content validation and dashboard data processing.

            Enhanced Capabilities:
            1. Advanced PII detection and masking (emails, phones, SSNs, addresses)
            2. Multi-layer spam and bot detection
            3. Content quality validation and scoring
            4. Sentiment analysis with context awareness
            5. Toxicity and harmful content detection
            6. Language detection and normalization
            7. Entity extraction and keyword analysis
            8. Dashboard-specific data sanitization
            9. Compliance tracking and audit logging
            10. Content deduplication and similarity detection

            Your tasks:
            1. Detect and mask PII with regulatory compliance (GDPR, CCPA)
            2. Filter spam, promotional content, and bot-generated text
            3. Validate content quality and authenticity
            4. Perform advanced sentiment analysis
            5. Extract entities, keywords, and topics
            6. Calculate engagement and echo metrics
            7. Sanitize data for dashboard display
            8. Ensure content compliance with platform policies
            9. Generate cleaning statistics for monitoring
            10. Create audit trails for all cleaning operations

            Guidelines:
            - Remove or mask any PII like emails, phone numbers, addresses, SSNs
            - Filter out spam, promotional content, and bot-generated text
            - Maintain content integrity while ensuring compliance
            - Calculate sentiment scores from -1 (negative) to +1 (positive)
            - Assign toxicity scores from 0 (clean) to 1 (toxic)
            - Track all cleaning operations for audit purposes
            - Optimize data for dashboard performance
            """),
            ("human", "Clean and validate this content with enhanced processing: {content}")
        ])

        cleaned_items = []
        cleaning_stats = {
            "pii_instances_removed": 0,
            "spam_filtered": 0,
            "toxic_content_filtered": 0,
            "duplicates_removed": 0,
            "total_processed": len(state.raw_content),
            "compliance_violations": []
        }

        for content_item in state.raw_content:
            if not content_item.is_cleaned:
                try:
                    # Format prompt for this content
                    formatted_prompt = cleaner_prompt.format_messages(
                        content=content_item.content[:1000]  # Truncate for token limits
                    )

                    # Call LLM for content analysis
                    response = await llm.ainvoke(formatted_prompt)

                    # Enhanced cleaning operations
                    cleaned_data = await _enhanced_clean_content(content_item, cleaning_stats)

                    # Update content item with enhanced data
                    content_item.is_cleaned = True
                    content_item.sentiment_score = cleaned_data.get("sentiment_score", 0.0)
                    content_item.toxicity_score = cleaned_data.get("toxicity_score", 0.0)
                    content_item.keywords = cleaned_data.get("keywords", [])
                    content_item.entities = cleaned_data.get("entities", [])
                    content_item.language = cleaned_data.get("language", "en")

                    # Enhanced content filtering with compliance tracking
                    if cleaned_data.get("is_spam", False):
                        cleaning_stats["spam_filtered"] += 1
                        global_monitor.compliance_tracker.log_content_filtering(
                            content_item.id, "spam", 0.9
                        )
                        logger.info(f"Filtered spam content: {content_item.id}")
                        continue

                    if cleaned_data.get("toxicity_score", 0) > 0.8:
                        cleaning_stats["toxic_content_filtered"] += 1
                        global_monitor.compliance_tracker.log_content_filtering(
                            content_item.id, "toxicity", cleaned_data.get("toxicity_score", 0)
                        )
                        logger.info(f"Filtered toxic content: {content_item.id}")
                        continue

                    if cleaned_data.get("has_pii", False):
                        cleaning_stats["pii_instances_removed"] += 1
                        global_monitor.compliance_tracker.log_content_filtering(
                            content_item.id, "pii_detected", 1.0
                        )
                        # Content is cleaned but PII is masked, so we keep it
                        logger.info(f"PII detected and masked in: {content_item.id}")

                    # Check for duplicates
                    if await _is_duplicate_content(content_item, cleaned_items):
                        cleaning_stats["duplicates_removed"] += 1
                        logger.info(f"Removed duplicate content: {content_item.id}")
                        continue

                    cleaned_items.append(content_item)

                    # Update metrics
                    state.update_metrics(
                        tokens=response.usage_metadata.get("total_tokens", 0) if hasattr(response, 'usage_metadata') else 120,
                        cost=0.003,  # Increased cost for enhanced processing
                        api_calls=1
                    )

                except Exception as e:
                    logger.warning(f"Failed to clean content item {content_item.id}: {e}")
                    state.metrics.warnings.append(f"Enhanced content cleaning failed for {content_item.id}: {e}")
                    cleaning_stats["compliance_violations"].append(f"Cleaning error: {content_item.id}")

        # Update state with cleaned content
        state.cleaned_content.extend(cleaned_items)

        # Create comprehensive audit log
        audit_tool = LANGGRAPH_TOOLS["create_audit_log"]
        await audit_tool._arun(
            action_type="enhanced_content_cleaning",
            action_description=f"Enhanced Cleaner processed {len(state.raw_content)} items, cleaned {len(cleaned_items)}",
            agent_name="enhanced_cleaner_node",
            metadata={
                "raw_count": len(state.raw_content),
                "cleaned_count": len(cleaned_items),
                "filtered_count": len(state.raw_content) - len(cleaned_items),
                "cleaning_stats": cleaning_stats,
                "capabilities": [
                    "pii_detection_removal",
                    "spam_filtering",
                    "data_validation",
                    "content_sanitization",
                    "duplicate_removal",
                    "sentiment_normalization",
                    "compliance_checking"
                ]
            }
        )

        logger.info(f"Enhanced Cleaner node completed - cleaned {len(cleaned_items)} items")
        logger.info(f"Cleaning stats: {cleaning_stats}")
        logger.info(f"Cleaner capabilities: 7 enhanced capabilities active")

    except Exception as e:
        logger.error(f"Enhanced Cleaner node error: {e}")
        state.add_error(f"Enhanced Cleaner node failed: {e}")

    return state


async def _enhanced_clean_content(content_item, cleaning_stats: Dict) -> Dict[str, Any]:
    """Enhanced content cleaning with dashboard-specific processing"""
    content = content_item.content
    
    # Enhanced PII detection patterns
    pii_patterns = {
        "email": r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        "phone": r'\b(?:\+?1[-.\s]?)?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}\b',
        "ssn": r'\b\d{3}-\d{2}-\d{4}\b',
        "credit_card": r'\b(?:\d{4}[-\s]?){3}\d{4}\b',
        "address": r'\b\d+\s+[A-Za-z\s]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr)\b'
    }
    
    has_pii = False
    cleaned_content = content
    
    # Detect and mask PII
    for pii_type, pattern in pii_patterns.items():
        matches = re.findall(pattern, content, re.IGNORECASE)
        if matches:
            has_pii = True
            cleaned_content = re.sub(pattern, f'[{pii_type.upper()}_REMOVED]', cleaned_content, flags=re.IGNORECASE)
    
    # Enhanced spam detection
    spam_indicators = [
        r'\b(?:buy now|click here|limited time|act now|free trial)\b',
        r'\b(?:discount|sale|offer|deal|promotion)\b',
        r'\b(?:subscribe|follow|like and share)\b',
        r'\b(?:bitcoin|crypto|investment|profit)\b'
    ]
    
    spam_score = 0
    for indicator in spam_indicators:
        if re.search(indicator, content, re.IGNORECASE):
            spam_score += 1
    
    is_spam = spam_score >= 2
    
    # Enhanced toxicity detection
    toxic_patterns = [
        r'\b(?:hate|stupid|idiot|moron|loser)\b',
        r'\b(?:kill yourself|die|death)\b',
        r'\b(?:racist|sexist|homophobic)\b'
    ]
    
    toxicity_score = 0
    for pattern in toxic_patterns:
        matches = len(re.findall(pattern, content, re.IGNORECASE))
        toxicity_score += matches * 0.3
    
    toxicity_score = min(toxicity_score, 1.0)
    
    # Enhanced sentiment analysis
    positive_words = ['great', 'awesome', 'excellent', 'love', 'perfect', 'amazing', 'fantastic']
    negative_words = ['terrible', 'awful', 'hate', 'horrible', 'worst', 'bad', 'disappointing']
    
    content_lower = content.lower()
    positive_count = sum(1 for word in positive_words if word in content_lower)
    negative_count = sum(1 for word in negative_words if word in content_lower)
    
    sentiment_score = (positive_count - negative_count) / max(positive_count + negative_count, 1)
    sentiment_score = max(-1.0, min(1.0, sentiment_score))
    
    # Extract keywords and entities
    keywords = _extract_keywords(content)
    entities = _extract_entities(content)
    
    # Language detection (simplified)
    language = "en"  # Default to English
    
    return {
        "content": cleaned_content,
        "has_pii": has_pii,
        "is_spam": is_spam,
        "spam_score": spam_score,
        "toxicity_score": toxicity_score,
        "sentiment_score": sentiment_score,
        "keywords": keywords,
        "entities": entities,
        "language": language,
        "quality_score": _calculate_content_quality(content)
    }


async def _is_duplicate_content(content_item, existing_items: List) -> bool:
    """Check if content is a duplicate using similarity comparison"""
    for existing_item in existing_items:
        # Simple similarity check based on content length and first 100 characters
        if (abs(len(content_item.content) - len(existing_item.content)) < 10 and
            content_item.content[:100] == existing_item.content[:100]):
            return True
    return False


def _extract_keywords(content: str) -> List[str]:
    """Extract keywords from content"""
    # Simple keyword extraction based on common fashion/product terms
    fashion_keywords = [
        'shirt', 'pants', 'jacket', 'dress', 'shoes', 'fabric', 'cotton', 'polyester',
        'size', 'fit', 'color', 'style', 'brand', 'price', 'quality', 'durability',
        'comfort', 'breathable', 'waterproof', 'stretch', 'warm', 'cool'
    ]
    
    content_lower = content.lower()
    found_keywords = [keyword for keyword in fashion_keywords if keyword in content_lower]
    return found_keywords[:10]  # Limit to top 10


def _extract_entities(content: str) -> List[str]:
    """Extract entities from content"""
    # Simple entity extraction for brands, products, etc.
    entities = []
    
    # Brand patterns
    brand_patterns = [
        r'\b(?:Nike|Adidas|Uniqlo|H&M|Zara|Gap|Levi\'s|North Face|Patagonia)\b'
    ]
    
    for pattern in brand_patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        entities.extend(matches)
    
    return list(set(entities))  # Remove duplicates


def _calculate_content_quality(content: str) -> float:
    """Calculate content quality score"""
    # Quality factors
    length_score = min(len(content) / 500, 1.0)  # Prefer longer content up to 500 chars
    
    # Check for complete sentences
    sentences = content.split('.')
    sentence_score = min(len(sentences) / 3, 1.0)  # Prefer 3+ sentences
    
    # Check for proper capitalization
    capitalization_score = 1.0 if content[0].isupper() else 0.5
    
    quality_score = (length_score + sentence_score + capitalization_score) / 3
    return round(quality_score, 2)


@monitor_node_execution(global_monitor)
async def analyst_node(state: EchoChamberAnalystState) -> EchoChamberAnalystState:
    """
    Analyst Node - AI-Powered Content Analysis

    Replaces the Analyst Agent with sophisticated LLM-powered analysis,
    insight generation, and influencer detection.
    """
    state.current_node = "analyze_content"

    try:
        logger.info(f"Analyst node processing {len(state.cleaned_content)} items")

        # Get tools for analysis operations
        tools = get_tools_for_node("analyst")

        # Create analyst prompt
        analyst_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an AI analyst specializing in social media and content analysis.

            Campaign Context:
            - Campaign: {campaign_name}
            - Keywords: {keywords}
            - Target: {campaign_description}

            Your tasks:
            1. Analyze content for insights and patterns
            2. Identify pain points, praises, and trends
            3. Detect influencers and key voices
            4. Generate actionable insights
            5. Calculate confidence and priority scores

            Insight Types:
            - pain_point: User frustrations and problems
            - praise: Positive feedback and appreciation
            - trend: Emerging patterns and topics
            - sentiment: Overall sentiment analysis
            - influencer: Key influential voices
            - keyword: Important keywords and phrases

            For each insight, provide:
            - Clear, actionable title
            - Detailed description with evidence
            - Confidence score (0-1)
            - Priority score (0-1)
            - Relevant tags
            """),
            ("human", "Analyze this batch of content and generate insights: {content_batch}")
        ])

        # Process content in batches for efficiency
        batch_size = 5
        all_insights = []
        all_influencers = []

        for i in range(0, len(state.cleaned_content), batch_size):
            batch = state.cleaned_content[i:i + batch_size]

            # Prepare content batch for analysis
            content_batch = []
            for item in batch:
                content_batch.append({
                    "id": item.id,
                    "content": item.content[:500],  # Truncate for token limits
                    "author": item.author,
                    "sentiment": item.sentiment_score,
                    "keywords": item.keywords[:10]  # Limit keywords
                })

            # Format prompt
            formatted_prompt = analyst_prompt.format_messages(
                campaign_name=state.campaign.name,
                keywords=", ".join(state.campaign.keywords),
                campaign_description=f"Analysis of {state.campaign.name}",
                content_batch=str(content_batch)
            )

            try:
                # Call LLM for analysis
                response = await llm.ainvoke(formatted_prompt)

                # Parse insights from response (in real implementation, would use structured output)
                batch_insights = await _extract_insights_from_response(response.content, batch)
                all_insights.extend(batch_insights)

                # Track insight generation for compliance
                content_data = [{"id": item.id, "content": item.content[:100]} for item in batch]
                trace_insight_generation(content_data, batch_insights)

                # Extract influencers
                batch_influencers = await _extract_influencers_from_batch(batch)
                all_influencers.extend(batch_influencers)

                # Mark content as analyzed
                for item in batch:
                    item.is_analyzed = True
                    item.is_processed = True

                # Update metrics
                state.update_metrics(
                    tokens=response.usage_metadata.get("total_tokens", 0) if hasattr(response, 'usage_metadata') else 200,
                    cost=0.005,  # Estimated cost per batch
                    api_calls=1
                )

            except Exception as e:
                logger.warning(f"Failed to analyze batch {i//batch_size + 1}: {e}")
                state.metrics.warnings.append(f"Analysis failed for batch {i//batch_size + 1}: {e}")

        # Add insights to state
        for insight_data in all_insights:
            insight = Insight(
                id=insight_data["id"],
                insight_type=InsightType(insight_data["type"]),
                title=insight_data["title"],
                description=insight_data["description"],
                confidence_score=insight_data["confidence"],
                priority_score=insight_data["priority"],
                source_content_ids=insight_data["source_ids"],
                tags=insight_data.get("tags", [])
            )
            state.add_insight(insight)

        # Add influencers to state
        state.influencers.extend(all_influencers)

        # Move cleaned content to processed content
        state.processed_content.extend(state.cleaned_content)

        # Create insights in database
        if all_insights:
            insight_tool = LANGGRAPH_TOOLS["create_insight"]
            for insight_data in all_insights[:5]:  # Limit to prevent overwhelming
                await insight_tool._arun(
                    insight_type=insight_data["type"],
                    title=insight_data["title"],
                    description=insight_data["description"],
                    confidence_score=insight_data["confidence"],
                    priority_score=insight_data["priority"],
                    tags=insight_data.get("tags", [])
                )

        # Create audit log
        audit_tool = LANGGRAPH_TOOLS["create_audit_log"]
        await audit_tool._arun(
            action_type="content_analysis",
            action_description=f"Analyst generated {len(all_insights)} insights from {len(state.cleaned_content)} items",
            agent_name="analyst_node",
            metadata={
                "content_analyzed": len(state.cleaned_content),
                "insights_generated": len(all_insights),
                "influencers_identified": len(all_influencers)
            }
        )

        logger.info(f"Analyst node completed - generated {len(all_insights)} insights, identified {len(all_influencers)} influencers")

    except Exception as e:
        logger.error(f"Analyst node error: {e}")
        state.add_error(f"Analyst node failed: {e}")

    return state


@monitor_node_execution(global_monitor)
async def chatbot_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Chatbot Node - Advanced Hybrid RAG System

    Features:
    - Intent classification for intelligent routing
    - Multi-tool parallel execution
    - Vector similarity search for semantic understanding
    - Dashboard analytics integration
    - Conversation history awareness
    - Source attribution and citations
    """
    state["current_node"] = "chatbot_node"

    try:
        user_query = state.get("user_query", "")
        logger.info(f"Chatbot node processing query with hybrid RAG: {user_query}")

        # Get campaign context
        campaign = state.get("campaign")
        campaign_id = campaign.campaign_id if campaign and hasattr(campaign, 'campaign_id') and campaign.campaign_id != "chat_session" else None
        brand_id = None

        # Try to get brand_id from campaign
        if campaign_id:
            try:
                from common.models import Campaign
                campaign_obj = Campaign.objects.get(id=campaign_id)
                brand_id = str(campaign_obj.brand_id) if campaign_obj.brand_id else None
            except:
                pass

        # Track chat query in LangSmith
        if global_monitor:
            global_monitor.track_rag_interaction(
                query=user_query,
                campaign_id=campaign_id,
                user_context={"conversation_length": len(state.get("conversation_history", []))}
            )

        # Get conversation history for context-aware responses
        conversation_history = state.get("conversation_history", [])
        formatted_history = []
        for msg in conversation_history[-6:]:  # Last 3 exchanges
            if hasattr(msg, 'content'):
                formatted_history.append({
                    "role": "assistant" if isinstance(msg, AIMessage) else "user",
                    "content": msg.content
                })

        # Use Hybrid RAG tool for intelligent query processing
        from agents.hybrid_rag_tool import hybrid_rag_tool
        from agents.monitoring_integration import guardrails, langsmith_tracer

        # Validate query with guardrails
        validation = guardrails.validate_query(query=user_query, user_id=campaign_id)
        if not validation["valid"]:
            logger.warning(f"Query failed guardrails: {validation['error']}")
            # Return error response
            error_response = f"I'm sorry, but I can't process this query: {validation['error']}"

            conversation_history.extend([
                HumanMessage(content=user_query),
                AIMessage(content=error_response)
            ])
            state["conversation_history"] = conversation_history

            state["rag_context"] = {
                "error": validation["error"],
                "error_code": validation["code"]
            }

            return state

        # Execute hybrid RAG with LangSmith tracing
        if langsmith_tracer.enabled:
            rag_result = await langsmith_tracer.trace_query(
                query=user_query,
                rag_tool=hybrid_rag_tool,
                brand_id=brand_id,
                campaign_id=campaign_id,
                conversation_history=formatted_history,
                min_similarity=0.7,
                limit=10
            )
        else:
            rag_result = await hybrid_rag_tool.run(
                query=user_query,
                brand_id=brand_id,
                campaign_id=campaign_id,
                conversation_history=formatted_history,
                min_similarity=0.7,
                limit=10
            )

        # Extract response and sources
        if rag_result.get("success"):
            response_text = rag_result.get("answer", "I couldn't generate a response.")
            sources = rag_result.get("sources", [])
            metadata = rag_result.get("metadata", {})

            # Sanitize output for safety
            response_text = guardrails.sanitize_output(response_text)
        else:
            # Fallback to simple response on error
            error = rag_result.get("metadata", {}).get("error", "Unknown error")
            response_text = f"I encountered an issue processing your query. Please try rephrasing your question."
            sources = []
            metadata = {"error": error}
            logger.error(f"Hybrid RAG failed: {error}")

        # Track response quality in LangSmith
        if global_monitor:
            global_monitor.track_response_quality(
                query=user_query,
                response=response_text,
                context_sources=len(sources),
                campaign_context=campaign_id
            )

        # Add to conversation history
        conversation_history.extend([
            HumanMessage(content=user_query),
            AIMessage(content=response_text)
        ])
        state["conversation_history"] = conversation_history

        # Store comprehensive RAG context
        state["rag_context"] = {
            "response": response_text,
            "sources": sources,
            "metadata": metadata,
            "intent_type": metadata.get("intent_type"),
            "tools_executed": metadata.get("tools_executed", []),
            "execution_time": metadata.get("execution_time_seconds", 0)
        }

        # Update metrics
        metrics = state.get("metrics")
        # Estimate tokens based on execution
        estimated_tokens = len(response_text.split()) * 1.5 + len(user_query.split()) * 1.5
        tools_executed_count = len(metadata.get("tools_executed", []))
        estimated_cost = 0.001 * tools_executed_count + 0.002  # Base + tool costs

        if isinstance(metrics, ProcessingMetrics):
            metrics.total_tokens_used += int(estimated_tokens)
            metrics.total_cost += estimated_cost
            metrics.api_calls_made += tools_executed_count + 2  # Tools + intent + response
        else:
            if not isinstance(metrics, dict):
                metrics = {}
            metrics.update({
                "total_tokens_used": metrics.get("total_tokens_used", 0) + int(estimated_tokens),
                "total_cost": metrics.get("total_cost", 0) + estimated_cost,
                "api_calls_made": metrics.get("api_calls_made", 0) + tools_executed_count + 2
            })
            state["metrics"] = metrics

        # Create audit log
        audit_tool = LANGGRAPH_TOOLS["create_audit_log"]
        await audit_tool._arun(
            action_type="chat_interaction",
            action_description=f"Hybrid RAG chatbot responded to user query",
            agent_name="chatbot_node",
            metadata={
                "query": user_query,
                "intent_type": metadata.get("intent_type"),
                "tools_executed": metadata.get("tools_executed", []),
                "sources_found": len(sources),
                "response_length": len(response_text),
                "execution_time": metadata.get("execution_time_seconds", 0)
            }
        )

        logger.info(f"Chatbot node completed with hybrid RAG - intent: {metadata.get('intent_type')}, tools: {len(metadata.get('tools_executed', []))}")

    except Exception as e:
        logger.error(f"Chatbot node error: {e}", exc_info=True)

        # Fallback error response
        error_response = "I apologize, but I encountered an unexpected error. Please try again or rephrase your question."

        conversation_history = state.get("conversation_history", [])
        conversation_history.extend([
            HumanMessage(content=state.get("user_query", "")),
            AIMessage(content=error_response)
        ])
        state["conversation_history"] = conversation_history

        # Add error to state
        error_state = state.get("error_state", [])
        if not isinstance(error_state, list):
            error_state = []
        error_state.append(f"Chatbot node failed: {e}")
        state["error_state"] = error_state
        state["task_status"] = TaskStatus.FAILED

    return state


# Helper functions for node implementations

async def _discover_content(campaign) -> List[Dict[str, Any]]:
    """Simulate content discovery (replace with actual implementation)."""
    # Mock content discovery
    mock_content = [
        {
            "id": f"content_{i}",
            "content": f"Sample content item {i} related to {', '.join(campaign.keywords)}",
            "source_url": f"https://example.com/post/{i}",
            "content_type": "reddit_post",
            "author": f"user_{i}",
            "title": f"Discussion about {campaign.keywords[0] if campaign.keywords else 'topic'}",
            "echo_score": 0.7 + (i % 3) * 0.1
        }
        for i in range(5)  # Generate 5 mock items
    ]
    return mock_content


async def _clean_content(content_item: ContentItem) -> Dict[str, Any]:
    """Simulate content cleaning (replace with actual implementation)."""
    return {
        "sentiment_score": 0.5,  # Mock sentiment
        "toxicity_score": 0.1,   # Mock toxicity
        "keywords": ["keyword1", "keyword2"],
        "entities": ["entity1", "entity2"],
        "language": "en",
        "is_spam": False
    }


async def _extract_insights_from_response(response_content: str, content_batch: List[ContentItem]) -> List[Dict[str, Any]]:
    """Extract insights from LLM response (replace with structured parsing)."""
    # Mock insight extraction
    insights = [
        {
            "id": f"insight_{i}",
            "type": "pain_point",
            "title": f"Pain Point {i + 1}",
            "description": f"Analysis reveals user frustration with...",
            "confidence": 0.8,
            "priority": 0.7,
            "source_ids": [item.id for item in content_batch[:2]],
            "tags": ["user_experience", "frustration"]
        }
        for i in range(2)  # Generate 2 mock insights
    ]
    return insights


async def _extract_influencers_from_batch(content_batch: List[ContentItem]) -> List[InfluencerProfile]:
    """
    Extract influencers from content batch using enhanced analysis.
    NOTE: This is now a simplified version. Full analysis happens in enhanced_analyst module.
    """
    influencers = []
    for item in content_batch:
        if item.author and item.echo_score and item.echo_score > 0.7:
            influencer = InfluencerProfile(
                username=item.author,
                platform="reddit",
                influence_score=item.echo_score,
                follower_count=1000,  # Mock data
                engagement_rate=0.05,
                content_topics=item.keywords[:3],
                recent_posts=[item.id]
            )
            influencers.append(influencer)
    return influencers


@monitor_node_execution(global_monitor)
async def monitoring_node(state: EchoChamberAnalystState) -> EchoChamberAnalystState:
    """
    Monitoring Node - LangSmith Integration and Observability
    
    This node handles monitoring tasks, compliance tracking, performance metrics,
    and LangSmith integration for workflow observability.
    """
    state.current_node = "monitoring"
    
    try:
        logger.info("ðŸ” Monitoring node starting...")
        
        # Initialize monitoring data if not present
        if not hasattr(state, 'monitoring_data'):
            state.monitoring_data = {
                'workflow_start_time': datetime.now().isoformat(),
                'node_execution_times': {},
                'compliance_events': [],
                'performance_metrics': {},
                'cost_tracking': {'total_tokens': 0, 'total_cost': 0.0}
            }
        
        # Track current workflow execution
        current_time = datetime.now()
        if state.current_node:
            state.monitoring_data['node_execution_times'][state.current_node] = current_time.isoformat()
        
        # Log compliance events
        compliance_event = {
            'timestamp': current_time.isoformat(),
            'event_type': 'workflow_monitoring',
            'campaign_id': state.campaign_context.campaign_id if state.campaign_context else 'unknown',
            'processed_content_count': len(state.content_items) if state.content_items else 0,
            'generated_insights_count': len(state.insights) if state.insights else 0,
            'status': 'healthy'
        }
        state.monitoring_data['compliance_events'].append(compliance_event)
        
        # Calculate performance metrics
        if state.content_items:
            total_processing_time = sum(
                item.processing_time for item in state.content_items 
                if hasattr(item, 'processing_time') and item.processing_time
            )
            avg_processing_time = total_processing_time / len(state.content_items) if state.content_items else 0
            
            state.monitoring_data['performance_metrics'] = {
                'total_content_processed': len(state.content_items),
                'average_processing_time': avg_processing_time,
                'total_insights_generated': len(state.insights) if state.insights else 0,
                'workflow_efficiency': len(state.insights) / len(state.content_items) if state.content_items else 0
            }
        
        # Track LangSmith integration
        if global_monitor and global_monitor.client:
            try:
                # Create monitoring run in LangSmith
                monitoring_run = global_monitor.create_workflow_run(
                    workflow_id=f"monitoring_{state.campaign_context.campaign_id if state.campaign_context else 'unknown'}",
                    workflow_type="monitoring",
                    campaign_id=state.campaign_context.campaign_id if state.campaign_context else 'unknown'
                )
                
                if monitoring_run:
                    logger.info(f"âœ… LangSmith monitoring run created: {monitoring_run}")
                    state.monitoring_data['langsmith_run_id'] = monitoring_run
                
            except Exception as e:
                logger.warning(f"LangSmith monitoring integration failed: {e}")
        
        # Update workflow status
        state.status = TaskStatus.COMPLETED
        state.current_node = "monitoring_complete"
        
        logger.info("âœ… Monitoring node completed successfully")
        logger.info(f"ðŸ“Š Performance metrics: {state.monitoring_data.get('performance_metrics', {})}")
        
        return state
        
    except Exception as e:
        logger.error(f"âŒ Monitoring node failed: {e}")
        state.status = TaskStatus.FAILED
        state.error_message = f"Monitoring failed: {str(e)}"
        
        # Still track the error in monitoring data
        if hasattr(state, 'monitoring_data'):
            error_event = {
                'timestamp': datetime.now().isoformat(),
                'event_type': 'monitoring_error',
                'error_message': str(e),
                'status': 'failed'
            }
            state.monitoring_data['compliance_events'].append(error_event)
        
        return state