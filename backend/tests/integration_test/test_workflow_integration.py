"""
Integration tests for end-to-end workflow execution.

Tests the complete agent workflow: Scout ‚Üí DataCleaner ‚Üí Analyst
"""
import pytest
import asyncio
from agents.orchestrator import EchoChamberWorkflowOrchestrator
from agents.state import create_initial_state, CampaignContext
from common.models import Campaign, RawContent, ProcessedContent, Insight, Influencer
from agents.tasks import scout_brand_analytics_task, scout_custom_campaign_task
from celery.result import AsyncResult
import time


@pytest.mark.integration
@pytest.mark.django_db(transaction=True)
class TestWorkflowIntegration:
    """Test end-to-end workflow execution."""

    @pytest.mark.asyncio
    async def test_complete_workflow_sync_execution(self, test_campaign_custom):
        """Test synchronous workflow execution through orchestrator."""
        # Create orchestrator
        orchestrator = EchoChamberWorkflowOrchestrator()

        # Verify orchestrator was created successfully
        assert orchestrator is not None
        assert orchestrator.graph is not None
        assert orchestrator.checkpointer is not None

        # Create initial state
        campaign_context = CampaignContext(
            campaign_id=str(test_campaign_custom.id),
            name=test_campaign_custom.name,
            keywords=test_campaign_custom.keywords,
            sources=test_campaign_custom.sources,
            budget_limit=float(test_campaign_custom.budget_limit)
        )

        state = create_initial_state(
            workflow_id=f"integration-test-{test_campaign_custom.id}",
            campaign=campaign_context
        )

        # Verify state was created correctly
        assert state is not None
        assert state["workflow_id"] == f"integration-test-{test_campaign_custom.id}"
        assert state["campaign"].campaign_id == str(test_campaign_custom.id)

        print(f"‚úÖ Workflow orchestrator and state created successfully for: {test_campaign_custom.name}")
        print(f"‚úÖ Campaign context: {campaign_context.name}")

        # Skip actual execution to avoid API costs in basic integration test
        # Full workflow execution is tested via Celery tasks below
        print("‚è≠Ô∏è  Skipping full workflow execution (tested via Celery tasks)")

    @pytest.mark.asyncio
    async def test_celery_workflow_execution_custom(self, test_campaign_custom):
        """Test async workflow execution through Celery for custom campaign."""
        print(f"\nüöÄ Triggering Celery task for custom campaign: {test_campaign_custom.name}")

        # Trigger Celery task - scout_custom_campaign_task expects int not string
        task = scout_custom_campaign_task.delay(test_campaign_custom.id)

        print(f"üìã Task ID: {task.id}")
        print(f"‚è≥ Waiting for task to complete (timeout: 300s)...")

        # Wait for task completion (with timeout)
        timeout = 300  # 5 minutes
        start_time = time.time()

        while time.time() - start_time < timeout:
            task_result = AsyncResult(task.id)

            if task_result.ready():
                if task_result.successful():
                    result = task_result.result
                    print(f"‚úÖ Task completed successfully!")
                    print(f"üìä Result: {result}")

                    # Verify result structure
                    assert result is not None

                    # Check if task was skipped (e.g., custom campaign without brand)
                    if isinstance(result, dict) and result.get('status') == 'skipped':
                        print(f"‚ö†Ô∏è  Task skipped: {result.get('reason')}")
                        print(f"‚úÖ Task completed (skipped is acceptable for custom campaigns without brand)")
                        return

                    assert "workflow_id" in result or "campaign_id" in result or "status" in result

                    # Refresh campaign and check metadata
                    test_campaign_custom.refresh_from_db()

                    # Check if data was collected (may be empty if APIs returned no results)
                    raw_content_count = RawContent.objects.filter(campaign=test_campaign_custom).count()
                    print(f"üì¶ Raw content collected: {raw_content_count}")

                    # Workflow should complete even if no data collected
                    return

                else:
                    # Task failed
                    print(f"‚ùå Task failed with error: {task_result.info}")
                    pytest.fail(f"Celery task failed: {task_result.info}")

            # Check every 5 seconds
            await asyncio.sleep(5)
            elapsed = time.time() - start_time
            print(f"‚è±Ô∏è  Still waiting... ({elapsed:.0f}s elapsed)")

        # Timeout reached
        pytest.fail(f"Task did not complete within {timeout}s timeout")

    @pytest.mark.asyncio
    async def test_celery_workflow_execution_brand_analytics(self, test_campaign_automatic):
        """Test async workflow execution through Celery for Brand Analytics campaign."""
        print(f"\nüöÄ Triggering Celery task for Brand Analytics: {test_campaign_automatic.name}")

        # Trigger Celery task - scout_brand_analytics_task expects brand_id not campaign_id
        # Use the brand.id from the campaign
        task = scout_brand_analytics_task.delay(test_campaign_automatic.brand.id)

        print(f"üìã Task ID: {task.id}")
        print(f"‚è≥ Waiting for task to complete (timeout: 300s)...")

        # Wait for task completion (with timeout)
        timeout = 300  # 5 minutes
        start_time = time.time()

        while time.time() - start_time < timeout:
            task_result = AsyncResult(task.id)

            if task_result.ready():
                if task_result.successful():
                    result = task_result.result
                    print(f"‚úÖ Task completed successfully!")
                    print(f"üìä Result: {result}")

                    # Verify result structure
                    assert result is not None

                    # Refresh campaign and check metadata
                    test_campaign_automatic.refresh_from_db()

                    # Check metadata for Brand Analytics insights
                    if test_campaign_automatic.metadata:
                        print(f"üí° Campaign metadata keys: {test_campaign_automatic.metadata.keys()}")

                    # Check if influencers were identified
                    influencer_count = Influencer.objects.filter(campaign=test_campaign_automatic).count()
                    print(f"üë• Influencers identified: {influencer_count}")

                    # Workflow should complete
                    return

                else:
                    # Task failed
                    print(f"‚ùå Task failed with error: {task_result.info}")
                    pytest.fail(f"Celery task failed: {task_result.info}")

            # Check every 5 seconds
            await asyncio.sleep(5)
            elapsed = time.time() - start_time
            print(f"‚è±Ô∏è  Still waiting... ({elapsed:.0f}s elapsed)")

        # Timeout reached
        pytest.fail(f"Task did not complete within {timeout}s timeout")

    def test_workflow_error_handling(self, test_user):
        """Test workflow handles invalid campaign gracefully."""
        from agents.tasks import scout_custom_campaign_task
        from celery.exceptions import TimeoutError
        import time

        # Try with non-existent campaign ID (using int 999999 which won't exist)
        task = scout_custom_campaign_task.delay(999999)

        # Wait for task to complete
        try:
            result = task.get(timeout=10)
            # If we get a result, verify it indicates an error/skip
            print(f"üìä Result for invalid campaign: {result}")
            assert isinstance(result, dict)
            # Task should handle gracefully (skip or error status)
            assert result.get('status') in ['skipped', 'error', 'failed'] or 'error' in str(result).lower()
            print(f"‚úÖ Invalid campaign handled gracefully: {result.get('status', 'error')}")
        except (TimeoutError, Exception) as e:
            # Task raised an exception or timed out - this is also acceptable
            print(f"‚úÖ Invalid campaign raised exception (acceptable): {type(e).__name__}")
            assert True  # Exception is acceptable for invalid campaign

    @pytest.mark.asyncio
    async def test_workflow_state_persistence(self, test_campaign_custom):
        """Test that workflow state is persisted correctly."""
        orchestrator = EchoChamberWorkflowOrchestrator()

        campaign_context = CampaignContext(
            campaign_id=str(test_campaign_custom.id),
            name=test_campaign_custom.name,
            keywords=test_campaign_custom.keywords,
            sources=test_campaign_custom.sources,
            budget_limit=float(test_campaign_custom.budget_limit)
        )

        state = create_initial_state(
            workflow_id=f"state-test-{test_campaign_custom.id}",
            campaign=campaign_context,
            config={"test_mode": True}
        )

        # Verify state structure
        assert state["workflow_id"] is not None
        assert state["campaign"] is not None
        assert state["config"]["test_mode"] is True

        # Verify checkpointer is configured
        assert orchestrator.checkpointer is not None
        print("‚úÖ State persistence configured correctly")
